{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campfire Burn Severity Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, balanced_accuracy_score, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "from scipy import interp\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campfire_df = pd.read_csv('dataset/campfire_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(paths.list_images('dataset/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in images:\n",
    "    obj_id = int(img_path.strip('dataset/OBJID_').strip('.tif'))\n",
    "    if obj_id in campfire_df.OBJECTID.values:\n",
    "        damage = campfire_df.loc[campfire_df.OBJECTID == obj_id].iloc[0].DAMAGE\n",
    "        os.rename('dataset/OBJID_{}.tif'.format(obj_id), 'dataset/{0}/OBJID_{1}.tif'.format(damage, obj_id))\n",
    "    else: \n",
    "        os.rename('dataset/OBJID_{}.tif'.format(obj_id), 'dataset/Unburned (0%)/OBJID_{}.tif'.format(obj_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "BS = 16\n",
    "IMAGE_DIMS = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, width, height, resample=None, random_state=0):\n",
    "    \"\"\"\n",
    "    Converts a dataset of images in the directory structure {CLASS_LABEL}/{FILENAME}.{IMAGE_EXTENSION} \n",
    "    to list of 3D NumPy arrays and their corresponding labels. Images resized to (width, height). Dataset \n",
    "    is resampled to balance class distribution based on resample='over'|'under'.\n",
    "    # Arguments\n",
    "        path: path to dataset \n",
    "        width: width of resized image \n",
    "        height: height of resized image \n",
    "        (optional) resample: resample dataset using ROS('over')/RUS('under')\n",
    "    # Returns\n",
    "        data: A list of 3D NumPy arrays converted from images \n",
    "        labels: A list of labels of the images \n",
    "    \"\"\"\n",
    "    image_paths = list(paths.list_images(path))\n",
    "    labels = [image_path.split(os.path.sep)[-2] for image_path in image_paths]\n",
    "    \n",
    "    if resample:\n",
    "        if resample == 'over':\n",
    "            sampler = RandomOverSampler(random_state=random_state)\n",
    "        elif resample == 'under':\n",
    "            sampler = RandomUnderSampler(random_state=random_state)\n",
    "        image_paths = [[image_path] for image_path in image_paths]\n",
    "        image_paths_resampled, labels = sampler.fit_resample(image_paths, labels)\n",
    "        image_paths = image_paths_resampled.ravel()\n",
    "\n",
    "    data = [img_to_array(load_img(img_path, target_size=(width, height))) for img_path in image_paths]    \n",
    "    \n",
    "    return np.array(data, dtype=\"float\") / 255.0, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = create_dataset('dataset/', IMAGE_DIMS[0], IMAGE_DIMS[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('dataset/data.npy', data)\n",
    "np.save('dataset/labels.npy', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = np.load('dataset/data.npy'), np.load('dataset/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(labels)\n",
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3)\n",
    "axs = axs.flatten()\n",
    "fig.set_size_inches((8, 8))\n",
    "for i, c in enumerate(classes):\n",
    "    axs[i].imshow(data[labels == c][np.random.choice(data[labels == c].shape[0], 1)[0]])\n",
    "    axs[i].set_title(c)\n",
    "fig.delaxes(axs[5])\n",
    "fig.delaxes(axs[4])\n",
    "for ax in axs:\n",
    "    ax.label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training/Testing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=0, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distribution(arr):\n",
    "    total = sum(Counter(arr).values())\n",
    "    return {c: c_count / total for c, c_count in Counter(arr).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniVGGNet:\n",
    "    \n",
    "    def __init__(self, name, input_shape, n_classes, init_lr, epochs, batch_size):\n",
    "        self.model = MiniVGGNet.build(input_shape=input_shape, n_classes=n_classes)\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        opt = Adam(lr=init_lr, decay=init_lr / epochs)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        model_filepath = 'model_checkpoints/{}/model.h5'.format(name)\n",
    "        mcp_save = ModelCheckpoint(model_filepath, save_best_only=True, monitor='val_loss', mode='min')\n",
    "        csv_logger = CSVLogger('model_checkpoints/{}/log.csv'.format(name))\n",
    "        self.callbacks = [mcp_save, csv_logger]\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_test, y_test):\n",
    "        return self.model.fit(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            batch_size=self.batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=self.epochs, \n",
    "            callbacks=self.callbacks)\n",
    "            \n",
    "    def fit_generator(self, X_train, y_train, X_test, y_test, generator, steps_per_epoch):\n",
    "        return self.model.fit_generator(\n",
    "            generator,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=self.epochs, \n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=self.callbacks)\n",
    "        \n",
    "    @staticmethod\n",
    "    def build(input_shape, n_classes):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding=\"same\", input_shape=input_shape, data_format='channels_last'))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(n_classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = MiniVGGNet('baseline', IMAGE_DIMS, n_classes, INIT_LR, EPOCHS, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.fit(X_train, y_train_bin, X_test, y_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = MiniVGGNet('baseline_ros', IMAGE_DIMS, n_classes, INIT_LR, EPOCHS, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros_generator, steps_per_epoch_ros = balanced_batch_generator(\n",
    "    X_train, \n",
    "    y_train_bin, \n",
    "    sampler=RandomOverSampler(),\n",
    "    batch_size=BS,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros.fit_generator(X_train, y_train_bin, X_test, y_test_bin, ros_generator, steps_per_epoch_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen = MiniVGGNet('baseline_datagen', IMAGE_DIMS, n_classes, INIT_LR, EPOCHS, BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_generator = ImageDataGenerator(\n",
    "    rotation_range=25, \n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True, \n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen.fit_generator(X_train, y_train_bin, X_test, y_test_bin, img_data_generator.flow(X_train, y_train_bin, batch_size=BS), X_train.shape[0] // BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_checkpoints/baseline_datagen/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_labels = np.argmax(y_test_pred, axis=-1)\n",
    "y_test_labels = np.argmax(y_test_bin, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_bin = []\n",
    "for l in y_test_pred_labels:\n",
    "    pred_bin = [0] * n_classes\n",
    "    pred_bin[l] = 1\n",
    "    y_test_pred_bin.append(pred_bin)\n",
    "y_test_pred_bin = np.array(y_test_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_labels, y_test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test_labels, y_test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_labels, y_test_pred_labels, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_confusion_matrix(y_test_labels, y_test_pred_labels, np.array(class_names), normalize=True, title='Datagen Model Confusion Matrix')\n",
    "plt.savefig('datagen_conf_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [re.sub(r' ?\\([^)]+\\)', '', c) for c in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "for i, c in enumerate(class_names):\n",
    "    fpr[c], tpr[c], _ = roc_curve(y_test_bin[:, i], y_test_pred_bin[:, i])\n",
    "    roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "    \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr['micro'], tpr['micro'], _ = roc_curve(y_test_bin.ravel(), y_test_pred_bin.ravel())\n",
    "roc_auc['micro'] = auc(fpr['micro'], tpr['micro'])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "lw = 2\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[c] for c in class_names]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for c in class_names:\n",
    "    mean_tpr += interp(all_fpr, fpr[c], tpr[c])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = ['aqua', 'darkorange', 'cornflowerblue', 'purple', 'green']\n",
    "for c, color in zip(class_names, colors):\n",
    "    plt.plot(fpr[c], tpr[c], color=color, lw=lw,\n",
    "             label='{0} (area = {1:0.2f})'\n",
    "             ''.format(c, roc_auc[c]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Datagen Multiclass ROC/AUC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('datagen_roc_auc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_train = get_class_weights(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
